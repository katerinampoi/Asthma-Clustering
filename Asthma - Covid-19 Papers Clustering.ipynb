{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering asthma-related papers in CORD-19 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The goal of this project is to explore research topics in asthma and coronviruses. What are the most popular topics the research community is focused on, before and after the COVID-19 outbreak? Are the areas of interest around asthma and coronaviruses the same before and after the appearance of SARS-CoV-2? \n",
    "\n",
    "In this project, I use Natural Language Processing (NLP) techniques in Python, to explore topics of research between asthma and coronaviruses before the identification of SARS-CoV-2, but also after the outbreak of the pandemic. The analysis is based on clustering scientific publications, in order to create groups of papers with similar topics. Two groups of clusters are created, one for papers published before and one for papers published after the COVID-19 outbreak. For the two periods of times, clustering aims at identifying popular research topics and finding potential gaps in research between asthma and the new coronavirus.\n",
    "\n",
    "More details about the motivation and the scientific background of this data analysis can be found in my Medium article:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "In response to the COVID-19 pandemic a large database, the COVID-19 Open Research Dataset (CORD-19), was created and has been made publicly available. CORD-19 is a resource of hundreds of thousands scholarly articles, about COVID-19, SARS-CoV-2, and related coronaviruses: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and Preprocessing Data\n",
    "\n",
    "From this large database, I kept only those papers where the word \"asthma\" appears at least once in the abstract.\n",
    "\n",
    "I went through the following text preprocessing steps, using NLTK and SpaCy:\n",
    "- Removal of stop words\n",
    "- Removal of non-English publications\n",
    "\n",
    "Using their publication date, I divided the papers into those published before the outbreak of the pandemic (December 2019) and those published after. For the two groups of papers I applied:\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Use of the Scikit-learn's Tfidf Vectorizer to transform tokens into a matrix of TF-IDF features\n",
    "- Application of the KMeans algorithm for Clustering\n",
    "- Application of the PCA algorithm for dimensionality reduction and clusters' visualization\n",
    "\n",
    "*Note: This data analysis was performed in February 2021 and doesn't take into account potential databases updates.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stop_words = ['whence', 'here', 'show', 'were', 'why', 'n’t', 'the', 'whereupon', 'not', 'more', 'how', 'eight', 'indeed', 'i', 'only', 'via', 'nine', 're', 'themselves', 'almost', 'to', 'already', 'front', 'least', 'becomes', 'thereby', 'doing', 'her', 'together', 'be', 'often', 'then', 'quite', 'less', 'many', 'they', 'ourselves', 'take', 'its', 'yours', 'each', 'would', 'may', 'namely', 'do', 'whose', 'whether', 'side', 'both', 'what', 'between', 'toward', 'our', 'whereby', \"'m\", 'formerly', 'myself', 'had', 'really', 'call', 'keep', \"'re\", 'hereupon', 'can', 'their', 'eleven', '’m', 'even', 'around', 'twenty', 'mostly', 'did', 'at', 'an', 'seems', 'serious', 'against', \"n't\", 'except', 'has', 'five', 'he', 'last', '‘ve', 'because', 'we', 'himself', 'yet', 'something', 'somehow', '‘m', 'towards', 'his', 'six', 'anywhere', 'us', '‘d', 'thru', 'thus', 'which', 'everything', 'become', 'herein', 'one', 'in', 'although', 'sometime', 'give', 'cannot', 'besides', 'across', 'noone', 'ever', 'that', 'over', 'among', 'during', 'however', 'when', 'sometimes', 'still', 'seemed', 'get', \"'ve\", 'him', 'with', 'part', 'beyond', 'everyone', 'same', 'this', 'latterly', 'no', 'regarding', 'elsewhere', 'others', 'moreover', 'else', 'back', 'alone', 'somewhere', 'are', 'will', 'beforehand', 'ten', 'very', 'most', 'three', 'former', '’re', 'otherwise', 'several', 'also', 'whatever', 'am', 'becoming', 'beside', '’s', 'nothing', 'some', 'since', 'thence', 'anyway', 'out', 'up', 'well', 'it', 'various', 'four', 'top', '‘s', 'than', 'under', 'might', 'could', 'by', 'too', 'and', 'whom', '‘ll', 'say', 'therefore', \"'s\", 'other', 'throughout', 'became', 'your', 'put', 'per', \"'ll\", 'fifteen', 'must', 'before', 'whenever', 'anyone', 'without', 'does', 'was', 'where', 'thereafter', \"'d\", 'another', 'yourselves', 'n‘t', 'see', 'go', 'wherever', 'just', 'seeming', 'hence', 'full', 'whereafter', 'bottom', 'whole', 'own', 'empty', 'due', 'behind', 'while', 'onto', 'wherein', 'off', 'again', 'a', 'two', 'above', 'therein', 'sixty', 'those', 'whereas', 'using', 'latter', 'used', 'my', 'herself', 'hers', 'or', 'neither', 'forty', 'thereupon', 'now', 'after', 'yourself', 'whither', 'rather', 'once', 'from', 'until', 'anything', 'few', 'into', 'such', 'being', 'make', 'mine', 'please', 'along', 'hundred', 'should', 'below', 'third', 'unless', 'upon', 'perhaps', 'ours', 'but', 'never', 'whoever', 'fifty', 'any', 'all', 'nobody', 'there', 'have', 'anyhow', 'of', 'seem', 'down', 'is', 'every', '’ll', 'much', 'none', 'further', 'me', 'who', 'nevertheless', 'about', 'everywhere', 'name', 'enough', '’d', 'next', 'meanwhile', 'though', 'through', 'on', 'first', 'been', 'hereby', 'if', 'move', 'so', 'either', 'amongst', 'for', 'twelve', 'nor', 'she', 'always', 'these', 'as', '’ve', 'amount', '‘re', 'someone', 'afterwards', 'you', 'nowhere', 'itself', 'done', 'hereafter', 'within', 'made', 'ca', 'them']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the list of stopwords taken into account\n",
    "stop_words.extend(spacy_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gitcord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>...</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "      <th>url</th>\n",
       "      <th>s2_id</th>\n",
       "      <th>abstract_lower</th>\n",
       "      <th>title_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qva0jt86</td>\n",
       "      <td>4ba79e54ecf81b30b56461a6aec2094eaf7b7f06</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Relevance of human metapneumovirus in exacerbations of COPD</td>\n",
       "      <td>10.1186/1465-9921-6-150</td>\n",
       "      <td>PMC1334186</td>\n",
       "      <td>16371156.0</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND AND METHODS: Human metapneumovirus (hMPV) is a recently discovered respiratory virus associated with bronchiolitis, pneumonia, croup and exacerbations of asthma. Since respiratory virus...</td>\n",
       "      <td>2005-12-21</td>\n",
       "      <td>...</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/4ba79e54ecf81b30b56461a6aec2094eaf7b7f06.json</td>\n",
       "      <td>document_parses/pmc_json/PMC1334186.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334186/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>background and methods: human metapneumovirus (hmpv) is a recently discovered respiratory virus associated with bronchiolitis, pneumonia, croup and exacerbations of asthma. since respiratory virus...</td>\n",
       "      <td>relevance of human metapneumovirus in exacerbations of copd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chz8luni</td>\n",
       "      <td>d68d71553d3a31381c0c3851351f912a9a7be1c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Surfactant therapy for acute respiratory failure in children: a systematic review and meta-analysis</td>\n",
       "      <td>10.1186/cc5944</td>\n",
       "      <td>PMC2206432</td>\n",
       "      <td>17573963.0</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>INTRODUCTION: Exogenous surfactant is used to treat acute respiratory failure in children, although the benefits and harms in this setting are not clear. The objective of the present systematic re...</td>\n",
       "      <td>2007-06-15</td>\n",
       "      <td>...</td>\n",
       "      <td>Crit Care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/d68d71553d3a31381c0c3851351f912a9a7be1c9.json</td>\n",
       "      <td>document_parses/pmc_json/PMC2206432.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2206432/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>introduction: exogenous surfactant is used to treat acute respiratory failure in children, although the benefits and harms in this setting are not clear. the objective of the present systematic re...</td>\n",
       "      <td>surfactant therapy for acute respiratory failure in children: a systematic review and meta-analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3zh8jmc2</td>\n",
       "      <td>fe2000f280297c40bc53ce95d703a9ca6aac19fd</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Differential Regulation of Type I Interferon and Epidermal Growth Factor Pathways by a Human Respirovirus Virulence Factor</td>\n",
       "      <td>10.1371/journal.ppat.1000587</td>\n",
       "      <td>PMC2736567</td>\n",
       "      <td>19806178.0</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>A number of paramyxoviruses are responsible for acute respiratory infections in children, elderly and immuno-compromised individuals, resulting in airway inflammation and exacerbation of chronic d...</td>\n",
       "      <td>2009-09-18</td>\n",
       "      <td>...</td>\n",
       "      <td>PLoS Pathog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/fe2000f280297c40bc53ce95d703a9ca6aac19fd.json</td>\n",
       "      <td>document_parses/pmc_json/PMC2736567.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2736567/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a number of paramyxoviruses are responsible for acute respiratory infections in children, elderly and immuno-compromised individuals, resulting in airway inflammation and exacerbation of chronic d...</td>\n",
       "      <td>differential regulation of type i interferon and epidermal growth factor pathways by a human respirovirus virulence factor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gitcord_uid                                       sha source_x  \\\n",
       "0    qva0jt86  4ba79e54ecf81b30b56461a6aec2094eaf7b7f06      PMC   \n",
       "1    chz8luni  d68d71553d3a31381c0c3851351f912a9a7be1c9      PMC   \n",
       "2    3zh8jmc2  fe2000f280297c40bc53ce95d703a9ca6aac19fd      PMC   \n",
       "\n",
       "                                                                                                                        title  \\\n",
       "0                                                                 Relevance of human metapneumovirus in exacerbations of COPD   \n",
       "1                         Surfactant therapy for acute respiratory failure in children: a systematic review and meta-analysis   \n",
       "2  Differential Regulation of Type I Interferon and Epidermal Growth Factor Pathways by a Human Respirovirus Virulence Factor   \n",
       "\n",
       "                            doi       pmcid   pubmed_id license  \\\n",
       "0       10.1186/1465-9921-6-150  PMC1334186  16371156.0   cc-by   \n",
       "1                10.1186/cc5944  PMC2206432  17573963.0   cc-by   \n",
       "2  10.1371/journal.ppat.1000587  PMC2736567  19806178.0   cc-by   \n",
       "\n",
       "                                                                                                                                                                                                  abstract  \\\n",
       "0  BACKGROUND AND METHODS: Human metapneumovirus (hMPV) is a recently discovered respiratory virus associated with bronchiolitis, pneumonia, croup and exacerbations of asthma. Since respiratory virus...   \n",
       "1  INTRODUCTION: Exogenous surfactant is used to treat acute respiratory failure in children, although the benefits and harms in this setting are not clear. The objective of the present systematic re...   \n",
       "2  A number of paramyxoviruses are responsible for acute respiratory infections in children, elderly and immuno-compromised individuals, resulting in airway inflammation and exacerbation of chronic d...   \n",
       "\n",
       "  publish_time  ...      journal mag_id  who_covidence_id arxiv_id  \\\n",
       "0   2005-12-21  ...   Respir Res    NaN               NaN      NaN   \n",
       "1   2007-06-15  ...    Crit Care    NaN               NaN      NaN   \n",
       "2   2009-09-18  ...  PLoS Pathog    NaN               NaN      NaN   \n",
       "\n",
       "                                                           pdf_json_files  \\\n",
       "0  document_parses/pdf_json/4ba79e54ecf81b30b56461a6aec2094eaf7b7f06.json   \n",
       "1  document_parses/pdf_json/d68d71553d3a31381c0c3851351f912a9a7be1c9.json   \n",
       "2  document_parses/pdf_json/fe2000f280297c40bc53ce95d703a9ca6aac19fd.json   \n",
       "\n",
       "                                 pmc_json_files  \\\n",
       "0  document_parses/pmc_json/PMC1334186.xml.json   \n",
       "1  document_parses/pmc_json/PMC2206432.xml.json   \n",
       "2  document_parses/pmc_json/PMC2736567.xml.json   \n",
       "\n",
       "                                                     url s2_id  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334186/   NaN   \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2206432/   NaN   \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2736567/   NaN   \n",
       "\n",
       "                                                                                                                                                                                            abstract_lower  \\\n",
       "0  background and methods: human metapneumovirus (hmpv) is a recently discovered respiratory virus associated with bronchiolitis, pneumonia, croup and exacerbations of asthma. since respiratory virus...   \n",
       "1  introduction: exogenous surfactant is used to treat acute respiratory failure in children, although the benefits and harms in this setting are not clear. the objective of the present systematic re...   \n",
       "2  a number of paramyxoviruses are responsible for acute respiratory infections in children, elderly and immuno-compromised individuals, resulting in airway inflammation and exacerbation of chronic d...   \n",
       "\n",
       "                                                                                                                  title_lower  \n",
       "0                                                                 relevance of human metapneumovirus in exacerbations of copd  \n",
       "1                         surfactant therapy for acute respiratory failure in children: a systematic review and meta-analysis  \n",
       "2  differential regulation of type i interferon and epidermal growth factor pathways by a human respirovirus virulence factor  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the csv file to a pandas dataframe, to have a look at the papers' metadata:\n",
    "asthma_df = pd.read_csv(\"asthma_data.csv\")\n",
    "asthma_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2567 papers containing the word \"asthma\", among the coronavirus-related publications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2567, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asthma_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of langdetect google library to dect the language of the \"abstract\" column in our dataframe\n",
    "\n",
    "asthma_df[\"lang_detect\"] = asthma_df[\"abstract_lower\"].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As non-english papers occupy only a small percent of our total papers, they are excluded from the analysis\n",
    "\n",
    "asthma_df[\"lang_detect\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2528 papers in total, written in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_df = asthma_df.loc[asthma_df['lang_detect'] == \"en\"]\n",
    "asthma_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to quickly vizualize the text of these papers, that's why I create a wordcloud where the most frequent words are represented by a bigger font! I'd like to give a shape to my image and use it as a cover for my article! What would it be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_mask = np.array(Image.open(\"coronavirus_canvas.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(ab for ab in asthma_df[\"abstract_lower\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=100, mask=virus_mask,\n",
    "               stopwords=stop_words, contour_width=3, contour_color='gray')\n",
    "\n",
    "wc.generate(text)\n",
    "\n",
    "wc.to_file(\"coronavirus_new.png\")\n",
    "\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to data processing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers are divided between those published before the identification of the new coronavirus, SARS-CoV-2, and those published after. I pick December 2019 as the cut-off date. \n",
    "\n",
    "The dataset contains 1023 papers published before December 2019 and 1507 papers published on December 2019 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid = asthma_df.loc[asthma_df['publish_time']<\"2019-12-01\"].reset_index(drop=True)\n",
    "asthma_before_covid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid = asthma_df.loc[asthma_df['publish_time']>=\"2019-12-01\"].reset_index(drop=True)\n",
    "asthma_after_covid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the number of the papers published, per month, since the covid outbreak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid['publish_time_new'] =  pd.to_datetime(asthma_after_covid['publish_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid['publish_month_year'] = pd.to_datetime(asthma_after_covid['publish_time']).dt.to_period('M')\n",
    "asthma_after_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid = asthma_after_covid.sort_values('publish_month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = asthma_after_covid[\"publish_month_year\"].value_counts()\n",
    "dates_df = dates.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = dates_df.sort_values(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df.rename(columns={\"index\": \"date_published\", \"publish_month_year\":\"number of papers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below illustrates the number of papers published through the last 12-14 months. However, we recognize that the two picks noticed in January 2020 and January 2021 are not completely accurate. As a certain number of papers had only the year (yyyy) mentioned as publication date, January 1st of that year (01/01/yyyy) is taken as their complete date.\n",
    "\n",
    "As a result, we cannot draw a very accurate example of the distribution of publications through the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df.plot(x ='index', y = 'publish_month_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_before = asthma_before_covid[\"abstract_lower\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(str_input):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    words = nltk.word_tokenize(str_input)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    words = [word.replace('â¡', '') for word in words]\n",
    "    words = [word.replace('â¢', '') for word in words]\n",
    "    words = [word.replace('â£', '') for word in words]\n",
    "       \n",
    "    words = [''.join(c for c in word if c not in string.punctuation+'©±×≤≥●＜--“”→„') for word in words]\n",
    "    words = [word for word in words if word not in ['‘', '’', '„']]\n",
    "        \n",
    "    words = [word for word in words if word]\n",
    "    words = [word for word in words if not any(char.isdigit() for char in word)]\n",
    "    \n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    words = [word for word in words if len(word)> 1]\n",
    "    words = [word for word in words if \"asthma\" not in word]\n",
    "    \n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_before = TfidfVectorizer(tokenizer=custom_tokenizer,\n",
    "                             max_features=2000,\n",
    "                      stop_words='english')\n",
    "\n",
    "matrix_before = vec_before.fit_transform(texts_before)\n",
    "df_before = pd.DataFrame(matrix_before.toarray(), columns=vec_before.get_feature_names())\n",
    "df_before.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the \"elbow\" method for an estimation of the optimal number of clusters for the group of papers:\n",
    "https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
    "\n",
    "However, in the context of this analysis the results are not reproducible and different number of clusters is suggested by the model. For this reason, the method is not taken completely into account. \n",
    "The final number of clusters for each group of papers is chosen after running the code for different numbers of clusters and reviewing the clusters' content (papers' topics) every time.\n",
    "\n",
    "For more information about the use of Kmeans in clustering: \n",
    "https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_elbow(data, min_cluster=4, max_cluster=20):\n",
    "    \n",
    "    model = KMeans(random_state=2)\n",
    "    visualizer = KElbowVisualizer(model, k=(min_cluster, max_cluster))\n",
    "\n",
    "    visualizer.fit(data)\n",
    "       \n",
    "    return visualizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_before = vizualize_elbow(matrix_before)\n",
    "viz_before.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters=11\n",
    "km_before = KMeans(n_clusters=number_of_clusters, random_state=1)\n",
    "model_before = km_before.fit(matrix_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model_before, open(\"model_before.pkl\", \"wb\"))\n",
    "#km_before = pickle.load(open(\"model_before.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an overview of our clusters' centers (centroids) and labels. Then we get the top 20 terms for every cluster. In other words, we see which are the most frequently mentioned words per cluster. Note that since we have applied Stemming, we only have the \"root\" of the words now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_before, labels_before = model_before.cluster_centers_, model_before.labels_\n",
    "print(labels_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids_before = centroids_before.argsort()[:, ::-1]\n",
    "terms = vec_before.get_feature_names()\n",
    "for i in range(number_of_clusters):\n",
    "    top_words = [terms[ind] for ind in order_centroids_before[i, :20]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and its implementation in Python:\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_before = preprocessing.Normalizer().fit_transform(df_before)\n",
    "\n",
    "# Fit and transform the TFidf values to PCA\n",
    "pca_model = PCA(n_components=2, random_state = 2)\n",
    "pca_model.fit(T_before) \n",
    "T_before = pca_model.transform(T_before)\n",
    "\n",
    "#Transform the centroids\n",
    "centroids_before_pca = pca_model.transform(centroids_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(centroids_before_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_before_df = pd.DataFrame(centroids_before_pca, columns = ['dimension 1','dimension 2'])\n",
    "centroids_before_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_before_df[\"pca_1\"] = centroids_before_pca[:,0]\n",
    "centroids_before_df[\"pca_2\"] = centroids_before_pca[:,1]\n",
    "centroids_before_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid['Labels'] = km_before.labels_\n",
    "asthma_before_covid['pca_1'] = T_before[:, 0]\n",
    "asthma_before_covid['pca_2'] = T_before[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid = asthma_before_covid.sort_values(by = \"Labels\", ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid['Labels'] = asthma_before_covid['Labels'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid['Labels'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_before = px.scatter(asthma_before_covid, \n",
    "                 x=\"pca_1\", \n",
    "                 y=\"pca_2\", \n",
    "                 color=\"Labels\",\n",
    "                 hover_data=['title'])\n",
    "\n",
    "fig_clusters_before.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at the clusters, we assing labels to them so that we can easily get a sense of each cluster's topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_before_titles = [\"Immune response\", \n",
    "                           \"Molecular links\",\n",
    "                           \"Asthma and COPD exacerbations\",\n",
    "                           \"title4\",\n",
    "                           \"title5\",                           \n",
    "                           \"title6\",\n",
    "                           \"title7\",                           \n",
    "                           \"title8\",\n",
    "                           \"title9\",\n",
    "                           \"title10\",\n",
    "                           \"title11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_before_df[\"centroids_labels\"] = centroids_before_titles\n",
    "centroids_before_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_before.add_scatter(y=centroids_before_df[\"pca_2\"].tolist(),\n",
    "                         x=centroids_before_df[\"pca_1\"].tolist(),\n",
    "                         mode=\"markers+text\",\n",
    "                         text=centroids_before_df[\"centroids_labels\"],\n",
    "                         marker=dict(size=10, color=\"white\"),\n",
    "                         name=\"Centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_before.update_layout(\n",
    "    title_text='Asthma and various coronaviruses',\n",
    "    legend=dict(\n",
    "        font=dict(\n",
    "            size=15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we also get the number of features, in our case the number of papers that each cluster contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_before_covid['Labels'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After covid-19\n",
    "\n",
    "The exact same process is followed for the groups of papers published after the SARS-CoV-2 outbreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_after = asthma_after_covid[\"abstract_lower\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_after = TfidfVectorizer(tokenizer=custom_tokenizer,\n",
    "                      stop_words='english', \n",
    "                           max_features=1000)\n",
    "\n",
    "matrix_after = vec_after.fit_transform(texts_after)\n",
    "df_after = pd.DataFrame(matrix_after.toarray(), columns=vec_after.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_after = vizualize_elbow(matrix_after)\n",
    "viz_after.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters=12\n",
    "km_after = KMeans(n_clusters=number_of_clusters, random_state=1)\n",
    "model_after = km_after.fit(matrix_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model_after, open(\"model_after.pkl\", \"wb\"))\n",
    "#km_after = pickle.load(open(\"model_after.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_after, labels_after = model_after.cluster_centers_, model_after.labels_\n",
    "print(centroids_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids_after = km_after.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vec_after.get_feature_names()\n",
    "for i in range(number_of_clusters):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids_after[i, :20]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_after = preprocessing.Normalizer().fit_transform(df_after)\n",
    "\n",
    "# Dimesionality reduction to 2\n",
    "pca_model = PCA(n_components=2, random_state=2)\n",
    "pca_model.fit(T_after) \n",
    "T_after = pca_model.transform(T_after) \n",
    "\n",
    "#Transform the cluster's centroids\n",
    "centroids_after_pca = pca_model.transform(centroids_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid['Labels'] = km_after.labels_\n",
    "asthma_after_covid['pca_1'] = T_after[:, 0]\n",
    "asthma_after_covid['pca_2'] = T_after[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_after_df = pd.DataFrame(centroids_after_pca, columns = ['dimension 1','dimension 2'])\n",
    "centroids_after_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_after_df[\"pca_1\"] = centroids_after_pca[:,0]\n",
    "centroids_after_df[\"pca_2\"] = centroids_after_pca[:,1]\n",
    "centroids_after_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid = asthma_after_covid.sort_values(by = \"Labels\", ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid['Labels'] = asthma_after_covid['Labels'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_after = px.scatter(asthma_after_covid, \n",
    "                 x=\"pca_1\", \n",
    "                 y=\"pca_2\", \n",
    "                 color=\"Labels\",\n",
    "                 hover_data=['title'])\n",
    "\n",
    "fig_clusters_after.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are assigned to thig group of clusters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_after_titles = [\"title 1\", \"title2\", \"title3\", \"title4\",\"title5\", \"title6\", \"title7\",\"title8\",\"title9\",\"title10\", \"title 12\", \"title13\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_after_df[\"centroids_labels\"] = centroids_after_titles\n",
    "centroids_after_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_after.add_scatter(y=centroids_after_df[\"pca_2\"].tolist(),\n",
    "                         x=centroids_after_df[\"pca_1\"].tolist(),\n",
    "                         mode=\"markers+text\",\n",
    "                         text=centroids_after_df[\"centroids_labels\"],\n",
    "                         marker=dict(size=10, color=\"white\"),\n",
    "                         name=\"Centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clusters_after.update_layout(\n",
    "    title_text='Asthma and SARS-CoV-2',\n",
    "    legend=dict(\n",
    "        font=dict(\n",
    "            size=15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asthma_after_covid['Labels'].value_counts().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
